\documentclass{article}

\title{Large-language models have no understanding of language.}
\date{October 16, 2023}
\author{L. L. Model}
\usepackage{lineno}
\linenumbers

\begin{document}
  \maketitle

Language, as a cornerstone of human existence, functions not merely as a
system of communication but as a means to convey thought, emotion, intent,
and a myriad of intricate nuances that encapsulate the human experience. 
The continuing journey of artificial intelligence, especially in the realm of
natural language processing, bears witness to a slew of advancements aimed at
making machines more adept at understanding and generating human language.
Large-language models, a culmination of this trajectory, are designed to process
and produce human-like textual content at an unprecedented scale. But does
proficiency in generation translate to understanding?
%todo: add second line here answering this question, or change this to be a better hypothesis

The definition of ”understanding” can vary. In operational terms, if a system can process inputs and produce outputs that align with expected outcomes
(like answering questions correctly), it could be argued that the system ”understands” the task at hand. By this metric, given the correct responses and contextually accurate text
generated by large-language models, it’s conceivable to argue that they possess an operational understanding of language. Understanding is defined as "knowledge about a subject, situation, etc. or about how something works" by the Cambridge Dictionary.
\cite{cambridgeUnderstanding}
This will be the definition used throughout this essay.

At their core, large-language models, like GPT series, operate by predicting
the likelihood of a word or sequence based on patterns identified in their training data. While these models are adept at generating coherent and contextually
relevant text, their operations are fundamentally statistical \cite{vapnik1998statistical}. They do not ”comprehend” language in the way humans do, wherein context, culture, emotion, and myriad other factors contribute to understanding. Instead, these models
recognize patterns and generate outputs based on those patterns, lacking any
genuine grasp of the underlying meaning or significance of the content. When comparing this fact to the definition of understanding mentioned above, it makes it clear that AI does not in fact possess knowledge about a subject. It is simply looking at statistics. This means it doesn't meet the criteria for understanding.

%But does that mean it doesn't understand language? Maybe elaborate why this is important for understanding
%also add connecting sentence to next paragraph
Human understanding of language is intrinsically tied to our sensory and experiential perceptions – words like ”red” or ”cold” have meanings rooted in our
tangible experiences \cite{barsalou2008grounded}.
One could say that humans gather knowledge about experiences and then are able to convey these experiences through language. Other humans, who have had similar experiences are then able to, at least partially, understand the others experience, without having been there. As humans, we use gathered knowledge to understand others, without having been there ourselves. On the other hand, Large-language models lack these sensory experiences; hence, their ”knowledge” of terms is purely informational, devoid of the rich
tapestry of associations and emotions humans naturally ascribe to such terms.
For instance, while a model can describe ”pain” based on data, it cannot truly
fathom the visceral, emotional, and physical experience that the term encapsulates for living beings.

%perhaps make this a continuation of the last point, as it is very similar
A foundational element of human understanding is intentionally – the capability to have beliefs, desires, fears, hopes, and more. Large-language models do
not possess desires or beliefs; they do not ”want” to generate text or ”believe”
in the validity of their outputs. Their operations are devoid of consciousness,
intention, or purpose, starkly contrasting with the human approach to language,
wherein every statement is underpinned by intent and understanding \cite{searle1980minds}.
While large-language models might not ”understand” language in a human-
like manner, their ability to mimic human-like text generation is profound.
These models can answer questions, generate narratives, \cite{radford2019language} and even craft poetry
that resonates with human readers. Such advanced mimicry can be perceived as
a form of understanding, blurring the lines between rote generation and genuine
comprehension.

    
Human understanding itself is not binary; it exists on a spectrum. A child’s
grasp of language differs from that of an adult. Similarly, if we consider understanding as a continuum, large-language models might occupy a space on
this spectrum, albeit not at the same depth as humans. Their vast data-driven
knowledge base provides them with an extensive, albeit shallow, grasp of language that might be perceived as a form of understanding. This being said, even a child's understanding of language is based on a basic understanding of the world around it. When a child says it is hungry, it knows what that means. One cannot say the same about a generative AI.

An important point to be considered in this discourse, is if LLM's understanding of language is even important. If they are so convincing, that no reasonable test could differentiate between a LLM and Human's text, then why should it matter if they are understanding what they are writing, outside of a theoretical context? The end user will not care, as long as the result they are receiving is convincing. This being said, an AI with no understanding of what it is saying can lead to dangerous outcomes. An AI might make dangerous recommendations, due to wrong training data. It could make offensive or derogatory remarks, due to not understanding what the things it is saying mean. This all becomes far more scary when we picture an Artificial Intelligence with a physical body. An AI with no understanding of what it is doing could quickly become dangerous, especially in situations that it has not been trained for.

On one hand, the mechanistic, pattern-driven nature of large language models and their lack of sensory experiences and intentionally clearly
demarcates them from human understanding. Yet, on the other hand, their
impressive ability to mimic human-like text generation, operational proficiency,
and the potential position on an ”understanding continuum” weave a convincing image of understanding. Large-language models, as marvels of technological advancement, offer a facsimile of human understanding, but one that is fundamentally distinct in its nature. While they push the boundaries of what machines can achieve in the realm of language, equating
their capabilities to genuine human comprehension might be an oversimplification. Yet, their position in the broader discourse on understanding cannot be ignored, serving as a testament to the intricacies of both human cognition and
machine learning.

\bibliographystyle{plain}
\bibliography{ref.bib} 
\end{document}
